\input{pre}
\geometry{a4paper}
\title{启发式搜索实验报告}
\author{武自厚 20336014 保密管理}
\date{\today}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\true}[0]{\textbf{\emph{true}}}
\newcommand{\false}[0]{\textbf{\emph{false}}}
\newcommand{\img}[3]{\begin{figure}[H]
    \centering
    \includegraphics[width=#1\textwidth]{#2}
\end{figure}}

\begin{document}
    \maketitle
    本次实验对A\(^*\)算法以及IDA\(^*\)算法进行分析并实现.

    \section{原理分析}

    \subsection{启发式函数}
    本次实验采用当前状态和目标状态对应的数字的Manhattan距离和作为启发函数,这是通过对于原问题进行松弛
    (即退化为两个数字无论如何都可以交换)而得来.即

    \[h(n) = \sum_{k=1}^{15} |x_n(k) - x_\text{dest}(k)|+|y_n(k) - y_\text{dest}(k)|\]
    
    需要注意的是``0''(即空位)的距离不应该加入结果,否则将会存在\(h(n) > h^*(n)\),也就是不保证找到
    最优解的情况.

    \subsection{A$^*$搜索}

    在该算法中,每一个状态\(n\)都对应着权值\(f(n)\),其满足
    \[f(n) = g(n) + h(n)\]
    其中\(g(n)\)定为从初始状态到状态\(n\)所需的步长,\(h(n)\)即是前文提到的启发式函数.

    该算法使用优先队列作为\texttt{Open}表维护,在算法开始时将初始状态推入优先队列,另外准备一个集合作为\texttt{Closed}表.
    当\texttt{Open}表不为空时,不断完成以下过程:

    \begin{enumerate}
        \item 从\texttt{Open}表中弹出\(n_k\),满足\(f(n_k) = \min_{n \in \texttt{Open}}f(n)\)
        \item 如果\(n_k\)就是目标状态,直接结束算法并返回结果.
        \item 遍历\(n_k\)所有可能的相邻状态(即行动一次就可以得到的状态)\(n_k^{(i)}\).
        如果\(n_k^{(i)} \notin \texttt{Closed}\),那么就计算出\(f(n_k^{(i)})\),并
        将\(n_k^{(i)}\)推入\texttt{Open}表.
    \end{enumerate}

    \subsection{IDA\(^*\)搜索}
    在该算法中,依旧存在映射\(f:n \mapsto f(n)\)表示状态的行动代价,其意义与
    A\(^*\)算法中的\(f\)相同.

    该算法使用栈作为\texttt{Open}表维护,在算法开始时将初始状态\(n_0\)压入栈,将初始成本边界定为\(b \leftarrow f(n_0)\).
    当\texttt{Open}表不为空时,不断完成以下过程:

    \begin{itemize}
        \item 从\texttt{Open}表中弹出\(n_k\).如果\(n_k\)就是目标状态,返回``已找到''信息;如果没有则继续.
        \item 如果\(f(n_k) > b\),那么直接返回\(f(n_k)\)
        \item 遍历\(n_k\)所有可能的相邻状态(即行动一次就可以得到的状态)\(n_k^{(i)}\).
        如果\(n_k^{(i)} \notin \texttt{Closed}\),那么就将\(n_k^{(i)}\)压入栈,执行递归算法.
        找出\(n_k^{(l)}\)使得\(f(n_k^{(l)}) = \min_{i} f (n_k^{(i)})\),返回\(f(n_k^{(l)})\).
    \end{itemize}

    如果最终没有返回``已找到''信息,则将返回值定为新的\(b\),再次执行算法.

    \section{效果分析}

    代码文件为父文件夹内\texttt{*.py}文件,会在标准输出中打印算法的\textit{处理器占用时间}以及处理的状态数量.

    \subsection{标准样例}

    样例1(小型):

    \img{1}{img/1}{}

    样例2(小型):

    \img{1}{img/2}{}

    样例3(大型):

    \img{1}{img/3}{}

    样例4(大型):

    \img{1}{img/4}{}

    可见,在深度不太大的情况下,A\(^*\)算法的时间复杂度优于IDA\(^*\)算法,而当深度很大时则是IDA\(^*\)算法更快.
    而IDA\(^*\)算法占用的空间远小于A\(^*\)算法.

    \subsection{不同启发函数的区别}

    这里采用两种启发函数进行对比:\(h_1(n)\)为Manhattan距离和,\(h_2(n)\)为错位牌的数量.
    \(h_1(n)\):

    \img{0.8}{img/1}{}
    
    \(h_2(n)\):

    \img{0.8}{img/5}{}

    由于\(h_2(n)\)中对问题的松弛程度比\(h_1(n)\)大,导致\(h_2(n) < h_1(n)\),算法需要更多步骤才能找到目标.

    \subsection{算法性能差距的原因}

    假设我们能够找到最优的启发式函数\(h^*(n)\),那么可以借助\(h^*(n)\)以最少的步数找到目标状态,
    因为采用\(h^*(n)\)时,算法只会按照最优路径进行扩展.
    然而在现实问题中我们很难直接找到\(h^*(n)\).只能通过各种情况对\(h^*(n)\)进行估计得到\(h(n)\).
    如果\(h(n) = 0\)也就是不采用启发式函数,那么算法将会退化为UCS甚至是BFS算法,虽然保证找到最短路径,但效率很低.
    如果\(h(n) > h^*(n)\),那么启发函数造成的效果将会被高估,将不会满足最优性.

    因此合适的启发式函数应当控制在\(0 < h(n) \le h^*(n)\)范围内.
    在这个范围内启发式函数都是\textit{可采纳的}且\textit{最优的}.
    但是如果太小,启发信息不足,算法需要扩展更多节点,造成性能下降;
    如果太大,算法需要花费更多资源去估计启发函数值,也会造成性能下降.
    因此一个合适的启发式函数更能优化算法.

    


\end{document}